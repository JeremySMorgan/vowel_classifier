
# smallest: 15
# m49er 22
# b27er 23
# w39oa 33
# b04oa 30
# w27er 22
# w04oa 39
# w41oa 27
# g08oa 29
# m31er 19
# w46er 36
# w03er 26
# m36oa 21
# m50er 24
# w20oa 30
# g11oa 34
# b03er 26
# m28er 18
# b01er 21
# g13oa 33
# b22oa 35
# m09oa 32
# m34oa 22
# w44er 30
# w01er 28
# w22oa 31
# m17er 24
# w25er 38
# m10oa 25
# b18er 28
# m33er 25
# w06oa 33
# b25er 26
# g14er 29
# w02oa 40
# w47oa 36
# m37er 19
# m14oa 25
# w21er 35
# g10er 30
# m29oa 22
# b02oa 32
# b21er 36
# b26oa 32
# m48oa 21
# w38er 28
# g17oa 24
# b05er 21
# m13er 25
# w26oa 36
# g09er 39
# w40er 28
# w05er 30
# m30oa 28
# w24oa 34
# m11er 32
# m32oa 18
# b19oa 31
# w42er 31
# w07er 30
# b24oa 20
# b07er 35
# w19oa 32
# g15oa 36
# g12er 25
# m08er 26
# b23er 31
# m35er 25
# w45oa 30
# w23er 32
# m16oa 19
# w23oa 32
# m16er 18
# m35oa 30
# w45er 29
# b23oa 29
# m08oa 26
# g12oa 22
# b07oa 29
# g15er 31
# w19er 30
# b24er 25
# b19er 26
# m32er 18
# w42oa 30
# w07oa 24
# w24er 31
# m11oa 35
# w40oa 28
# w05oa 36
# g09oa 40
# m30er 29
# m13oa 25
# w26er 32
# g17er 25
# b05oa 23
# m48er 23
# b26er 33
# w38oa 28
# b21oa 38
# g10oa 32
# b02er 37
# m29er 19
# m14er 27
# w21oa 35
# w02er 40
# w47er 31
# m37oa 19
# g14oa 31
# b25oa 27
# m33oa 24
# b18oa 24
# w06er 32
# w25oa 42
# m10er 23
# w22er 26
# m17oa 25
# m34er 23
# w44oa 33
# w01oa 28
# m09er 32
# b22er 32
# b01oa 25
# g13er 27
# g11er 28
# m28oa 19
# b03oa 29
# m50oa 23
# w20er 30
# w46oa 33
# w03oa 25
# m36er 27
# g08er 33
# w04er 33
# w41er 24
# m31oa 15
# w27oa 24
# b04er 37
# b27oa 24
# m49oa 17
# w39er 32
# w16oa 33
# b08er 24
# m23er 25
# m45oa 23
# w35er 28
# w08er 25
# g04er 34
# b16oa 29
# m19oa 23
# g20er 27
# b11er 30
# m07er 25
# m42er 31
# w32oa 22
# w11er 37
# m24oa 21
# w30oa 29
# m40er 25
# m26oa 25
# w13er 25
# b13er 23
# m38er 19
# g01oa 24
# w48oa 26
# b14oa 24
# g06er 28
# w29oa 26
# m21er 16
# g18oa 28
# w14oa 24
# w37er 35
# b29oa 25
# m47oa 27
# m02oa 24
# m18er 28
# g21oa 23
# g02er 24
# b10oa 30
# m06oa 24
# w33er 32
# w10oa 34
# m25er 24
# w17er 29
# m22oa 27
# b09oa 19
# m01er 30
# m44er 23
# w34oa 23
# g05oa 36
# w09oa 24
# b17er 28
# b15er 35
# g07oa 42
# w28er 20
# m20oa 30
# w50er 34
# w15er 33
# g19er 18
# w36oa 32
# m03er 35
# b28er 24
# w31er 31
# m41oa 21
# m04oa 28
# m27er 21
# w12oa 30
# m39oa 23
# b12oa 38
# w49er 28
# b12er 35
# m39er 24
# w49oa 29
# m27oa 23
# w12er 33
# w31oa 34
# m41er 19
# m04er 31
# w36er 42
# b28oa 27
# m03oa 40
# m20er 34
# g19oa 20
# w50oa 34
# w15oa 34
# w28oa 20
# b15oa 37
# g07er 34
# w09er 24
# g05er 33
# b17oa 27
# m01oa 35
# m44oa 26
# w34er 22
# w17oa 28
# b09er 24
# m22er 24
# w10er 32
# m25oa 24
# m06er 20
# w33oa 26
# g02oa 27
# b10er 31
# m18oa 32
# g21er 24
# w37oa 39
# m47er 24
# m02er 28
# b29er 37
# m21oa 18
# w14er 25
# g18er 27
# w29er 29
# b14er 26
# g06oa 25
# m38oa 19
# b13oa 24
# w48er 33
# g01er 28
# m26er 23
# w13oa 29
# w30er 31
# m40oa 24
# w11oa 35
# m24er 24
# m07oa 25
# m42oa 27
# w32er 25
# b11oa 25
# m19er 22
# g20oa 30
# g04oa 36
# w08oa 28
# b16er 31
# m45er 23
# w35oa 29
# w16er 27
# m23oa 26
# b08oa 32

class MFCC_Generator(object):

    def __init__(self, sound_file, start_end_t):

        self.sound_file = sound_file
        self.start_t = start_end_t[0]
        self.end_t = start_end_t[1]
        self.calculated

    def calculate_mfc_coeffs(self, normalize=True):

        # Constants
        pre_emphasis = .97
        frame_stride = 0.01  # 0.01     # 10 ms overlap of dft windows
        frame_size = .025  # 0.025      # 25 ms for dft window size
        NFFT = 512  # FFT bin size
        nfilt = 40  # Number of mel filters
        num_ceps = 12
        sample_time = 3.5  # how long to read from audio
        cep_lifter = 22

        # ae_wav_file = "vowels/men/m01ae.wav"

        sample_rate, signal = scipy.io.wavfile.read(self.sound_file)
        sample_rate = float(sample_rate)

        signal = signal[ int( self.start_t * sample_rate) : int(self.end_t * sample_rate) ]  # Keep the first 3.5 seconds

        # y(t) = x(t) - pre_emphasis*x(t-1)
        emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])

        frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples
        signal_length = len(emphasized_signal)
        frame_length = int(round(frame_length))
        frame_step = int(round(frame_step))
        num_frames = int(np.ceil(
            float(np.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame

        pad_signal_length = num_frames * frame_step + frame_length
        z = np.zeros((pad_signal_length - signal_length))
        pad_signal = np.append(emphasized_signal,
                               z)  # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal

        indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(
            np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T
        frames = pad_signal[indices.astype(np.int32, copy=False)]
        frames *= np.hamming(frame_length)

        mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude of the FFT
        pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum

        # See http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/ for below
        low_freq_mel = 0
        high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel
        mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale
        hz_points = (700 * (10 ** (mel_points / 2595) - 1))  # Convert Mel to Hz
        bin = np.floor((NFFT + 1) * hz_points / sample_rate)

        fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))
        for m in range(1, nfilt + 1):
            f_m_minus = int(bin[m - 1])  # left
            f_m = int(bin[m])  # center
            f_m_plus = int(bin[m + 1])  # right

            for k in range(f_m_minus, f_m):
                fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])
            for k in range(f_m, f_m_plus):
                fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])
        filter_banks = np.dot(pow_frames, fbank.T)
        filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Numerical Stability
        filter_banks = 20 * np.log10(filter_banks)  # dB

        # Apply Discrete Cosine Transformation
        mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, 1: (num_ceps + 1)]  # Keep 2-num_ceps+1
        (nframes, ncoeff) = mfcc.shape
        n = np.arange(ncoeff)
        lift = 1 + (cep_lifter / 2) * np.sin(np.pi * n / cep_lifter)
        mfcc *= lift  # *
        mfcc -= (np.mean(mfcc, axis=0) + 1e-8)

        if normalize:
            mfcc / np.max(np.abs(mfcc))
        # filter_banks -= (np.mean(filter_banks, axis=0) + 1e-8)

        # print "  sample rate:", sample_rate
        # print "  frame_size:", frame_size
        # print "  frame_length:", frame_length
        # print "  num_frames:", num_frames
        # print "  filter banks shape:", filter_banks.shape
        # print "  mfcc shape:", mfcc.shape

        # plt.figure(1)
        # plt.subplot(211)
        # plt.imshow(filter_banks.T, cmap=plt.cm.jet, aspect='auto')
        # plt.title('normalized filter banks')
        # plt.subplot(212)
        # plt.imshow(mfcc.T, cmap=plt.cm.jet, aspect='auto')
        # plt.title('normalized mfcc coeffs')
        # plt.show()

        # if plot_d_emphasize_t_domain:
        #     plt.plot(signal)
        #     plt.xlabel('time (s)')
        #     plt.ylabel('signal amplitude')
        #     plt.title('Time vs Amplitude of Original Signal')
        #     plt.grid(True)
        #     # plt.savefig("test.png")
        #     plt.ion()
        #     plt.draw()
        #
        #
        # if plot_t_domain:
        #     plt.plot(signal)
        #     plt.xlabel('time (s)')
        #     plt.ylabel('signal amplitude')
        #     plt.title('Time vs Amplitude of Original Signal')
        #     plt.grid(True)
        #     # plt.savefig("test.png")
        #     plt.ion()
        #     plt.draw()
        #
        # scipy.io.wavfile.write('test.wav', sample_rate, signal)

        # plt.show()

        return mfcc



    # all methods: ['LinearSVC',  'LinearSVR',   'NuSVC', 'NuSVR', 'SVC', 'SVR', "LDA"
    # LinearSVC and LinearSVR perform very poorly
    # LDA is not working as expected, only predicts '1'

    # for method in [ 'NuSVC', 'NuSVR', 'SVC', 'SVR']:
    #     trainer = SciKit_Trainer(training_vowels)
    #     test_method(trainer, method, training_vowels)

    # trainer = SciKit_Trainer(training_vowels)
    # trainer.train_predict()

    # --------------------------------------------------------------------------------------------- tensorflow classification

class SciKit_Trainer(object):

    time_data_csv_file = 'vowels/timedata.csv'

    # ae
    # ah
    # aw
    # eh
    # er
    # ih
    # iy
    # oa
    # oo
    # uh
    # uw

    vowels = [ "ae", "ah", "aw", "eh", "ei", "er", "ih", "iy", "oa", "oo", "uh", "uw" ]

    def __init__(self, training_vowels):

        self.mcf_coeffs = []
        self.classifier = []

        self.training_vowels = training_vowels
        self.training_vowels_accuracy = [] # [[vowel 1 correct cnt, vowel 1 err count],[vowel2 correct cnt, vowel2 err count],..]

        self.mfcc_generator = MFCC_Generator()

    def get_vowels(self):
        return self.training_vowels


    def train_predict(self, pct_to_train=.9, method="SVC"):

        directory = "vowels/audioclips"

        # only add if # > min_num_to_test
        rand_int_min = 0

        for sound_name in os.listdir(directory):

            sound_name_no_wav = sound_name.split('.')[0]
            file_name = directory+"/"+sound_name

            for vowel in self.training_vowels:

                if vowel in sound_name_no_wav and int(sound_name_no_wav[1:3]) > (1.0-pct_to_train)*50:

                    # start_end_t = self.csv_reader.get_start_end_time_from_fname(file_name)

                    ret = self.mfcc_generator.calculate_mfc_coeffs(file_name)

                    # data = self.get_filtered_data(ret)
                    # data = []
                    # print len(ret)

                    # self.mcf_coeffs.append(data)
                    # classifier_num = self.training_vowels.index(vowel)
                    # self.classifier.append(classifier_num)

                    for mcf_coeff in ret:
                        if random.randint(0,1000) > rand_int_min:
                            self.mcf_coeffs.append(mcf_coeff)
                            classifier_num = self.training_vowels.index(vowel)
                            self.classifier.append(classifier_num)

        # Scikit requires 2d input. Workarounds:
        #        - stack all coeffs into one array/ sound

        # print(len(self.mcf_coeffs))
        # print(self.mcf_coeffs[0])
        # print("\n")
        # print(self.mcf_coeffs[1])
        # print("\n ")
        # print(self.mcf_coeffs[2])

        if method == "SVC":
            clf = svm.SVC(gamma='scale')
        elif method == "LDA":
            clf = LinearDiscriminantAnalysis(solver='lsqr')
        elif method == "SVR":
            clf = svm.SVC(gamma='scale')
        elif method == 'LinearSVC':
            clf = svm.LinearSVC()
        elif method == 'LinearSVR':
            clf = svm.LinearSVR()
        elif method == 'NuSVC':
            clf = svm.NuSVC(gamma='scale')
        elif method == 'NuSVR':
            clf = svm.NuSVR(gamma='scale')
        # elif method == 'OneClassSVM':
        #     clf = svm.OneClassSVM(gamma='scale')
        else:
            return


        clf.fit(self.mcf_coeffs, self.classifier)

        for _ in self.training_vowels:
            self.training_vowels_accuracy.append([0,0])

        for sound_name in os.listdir(directory):

            sound_name_no_wav = sound_name.split('.')[0]
            file_name = directory+"/"+sound_name

            for vowel in self.training_vowels:

                if vowel in sound_name_no_wav and int(sound_name_no_wav[1:3]) < (1.0-pct_to_train)*50:

                    start_end_t = self.csv_reader.get_start_end_time_from_fname(file_name)
                    mfcc_generator = MFCC_Generator(file_name, start_end_t)
                    ret = mfcc_generator.calculate_mfc_coeffs()
                    classifier_num = self.training_vowels.index(vowel)

                    # data = self.get_filtered_data(ret)
                    # prediction = clf.predict(data)
                    # print(prediction)
                    #
                    # prediction = prediction[0]
                    #
                    # #
                    # # # continuous prediction -> need to map
                    # if not prediction == int(prediction):
                    #     # print "original prediction:",prediction
                    #     best_fitting_vowel_id = -1
                    #     best_fitting_vowel_dist = 1000
                    #     for i in range(len(self.training_vowels)):
                    #         if np.abs(i-prediction) < best_fitting_vowel_dist:
                    #             best_fitting_vowel_dist = np.abs(i-prediction)
                    #             best_fitting_vowel_id = i
                    #     prediction = best_fitting_vowel_id
                    #     # print "mapped prediction:",prediction
                    # if prediction == classifier_num:
                    #     self.training_vowels_accuracy[classifier_num][0] += 1
                    # else:
                    #     self.training_vowels_accuracy[classifier_num][1] += 1
                    #

                    for coeff in ret:

                        prediction = clf.predict([coeff])[0]

                        # continuous prediction -> need to map
                        if not prediction == int(prediction):
                            # print "original prediction:",prediction
                            best_fitting_vowel_id = -1
                            best_fitting_vowel_dist = 1000
                            for i in range(len(self.training_vowels)):
                                if np.abs(i-prediction) < best_fitting_vowel_dist:
                                    best_fitting_vowel_dist = np.abs(i-prediction)
                                    best_fitting_vowel_id = i
                            prediction = best_fitting_vowel_id
                            # print "mapped prediction:",prediction

                        if prediction == classifier_num:
                            self.training_vowels_accuracy[classifier_num][0] += 1
                        else:
                            self.training_vowels_accuracy[classifier_num][1] += 1

                    #     # print vowel,": prediction:",prediction,"\tcorrect:",classifier_num,"training vowel accuracy:",self.training_vowels_accuracy
                    #     # time.sleep(.1)

        return self.training_vowels_accuracy
